{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "c7g43tDcf92S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **seed 고정**"
      ],
      "metadata": {
        "id": "P62BYcLRgOcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "hqP9wvkogOm2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU"
      ],
      "metadata": {
        "id": "rrDSkANGKARF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8XUH9ZlKAVV",
        "outputId": "0685aac3-9da9-40f6-8747-36ff88cede84"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging face dateset가져오기**"
      ],
      "metadata": {
        "id": "UhR6V876g0H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face datasets 라이브러리 설치 (설치되지 않은 경우)\n",
        "!pip install datasets\n",
        "\n",
        "# GLUE 데이터셋 불러오기\n",
        "from datasets import load_dataset\n",
        "\n",
        "# MRPC (Microsoft Research Paraphrase Corpus) 태스크 로드 예시\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")"
      ],
      "metadata": {
        "id": "cVuuqi9Mg0oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb128e0-3759-4c29-e17f-cdd184d1793c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 정보 출력\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_l1c0qFhRWy",
        "outputId": "6cdf38ab-835a-4b18-dff4-13678e4b4e97"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence1 , sentence2의 동의어 관계 판별 ,\n",
        "\n",
        "---\n",
        "label : 1 -> 동의어 관계\n",
        "label : 0 -> 동의어 관계 아님\n",
        "\n",
        "---\n",
        "idx : 고유 id\n",
        "\n"
      ],
      "metadata": {
        "id": "o-M7clpairzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련, 검증 데이터 예시 출력\n",
        "print(f\"Train Example: {dataset['train'][0]}\")\n",
        "print(f\"Validation Example: {dataset['validation'][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIEBvV71hPv7",
        "outputId": "b60145f0-5f19-45f4-d800-739cdce00839"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Example: {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
            "Validation Example: {'sentence1': \"He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\", 'sentence2': '\" The foodservice pie business does not fit our long-term growth strategy .', 'label': 1, 'idx': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰화**"
      ],
      "metadata": {
        "id": "0ZkG9Grohqw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. BERT 토크나이저 불러오기 (사전 학습된 BERT 모델 사용)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GusHkmBihq6w",
        "outputId": "9ca3a662-d8cd-4f83-add8-e5c7b0f5e094"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 토큰화 함수 정의\n",
        "def tokenize_function(examples):\n",
        "    # sentence1과 sentence2를 함께 토큰화 (Padding, Truncation 처리)\n",
        "    return tokenizer(examples[\"sentence1\"],\n",
        "                     examples[\"sentence2\"],\n",
        "                     padding=\"max_length\",\n",
        "                     truncation=True,\n",
        "                     max_length=128)"
      ],
      "metadata": {
        "id": "RUfi08-xkTpm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 데이터셋에 토큰화 적용\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "DHk1hnGjkVX_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 토큰화된 데이터 예시 출력\n",
        "print(tokenized_dataset[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLrWK7eakcPk",
        "outputId": "fab890cf-5f93-4ecb-e303-5d0ed5ca38e1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0, 'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구조 :\n",
        "\n",
        "---\n",
        "\n",
        "(sentence1 원문 , sentence2 원문 , label , idx , input_ids1 & 2 , token_type_ids , attention_mask)\n",
        "\n",
        "---\n",
        "token_type_ids : 두 문장(문장1, 문장2)을 구분하기 위한 인코딩 (BERT에서 사용),\n",
        "\n",
        "---\n",
        "\n",
        "attention_mask : 패딩된 부분을 무시하도록 마스킹 처리 (1은 유효한 토큰, 0은 패딩)\n"
      ],
      "metadata": {
        "id": "bB3oefQyly8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **배치화**"
      ],
      "metadata": {
        "id": "k5Dnu5gIk2bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 DataLoader에 넣기\n",
        "train_loader = DataLoader(tokenized_dataset['train'], batch_size=16, shuffle=True , drop_last=True)"
      ],
      "metadata": {
        "id": "KK-gKR9uk8u9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 배치 반복 및 크기 확인\n",
        "for batch in train_loader:\n",
        "    print(batch.keys())  # 딕셔너리의 키 확인 (예: input_ids, attention_mask, label)\n",
        "\n",
        "    input_ids = batch['input_ids']\n",
        "    print(f\"Input IDs shape: {len(input_ids)}\")\n",
        "    print(f\"Input IDs shape: {len(input_ids[0])}\")\n",
        "\n",
        "    attention_mask = batch['attention_mask']\n",
        "    print(f\"Attention Mask shape: {len(attention_mask)}\")\n",
        "    print(f\"Attention Mask shape: {len(attention_mask[0])}\")\n",
        "    break  # 첫 배치만 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSph_f_kk4yZ",
        "outputId": "cdfc2de0-7293-4791-b9d6-9f0c117b8739"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
            "Input IDs shape: 128\n",
            "Input IDs shape: 16\n",
            "Attention Mask shape: 128\n",
            "Attention Mask shape: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **특정 배치에서 sequence_length가 4로 나머지 16과 불일치**"
      ],
      "metadata": {
        "id": "n2HHmi6bTmoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **필드 추출**"
      ],
      "metadata": {
        "id": "gF-Q8ZFE2azs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. DataLoader에서 배치 추출 및 Encoder에 입력 예시\n",
        "for batch in train_loader:\n",
        "    # 필요한 텐서 추출\n",
        "    input_ids = batch['input_ids']\n",
        "    # 어디 문장 소속인지 여부를 표시하는 텐서 추가\n",
        "    token_type_ids = batch['token_type_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "\n",
        "    print(f\"Batch size: {len(input_ids)}, Sequence length: {len(input_ids[0])}\")"
      ],
      "metadata": {
        "id": "p11P7J3-2aAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40c76ee-4cd4-480a-b1ed-7ad61338ea01"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n",
            "Batch size: 128, Sequence length: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터 처리**"
      ],
      "metadata": {
        "id": "Yne_BbbVny3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch는 dictionary 형태이므로 필요한 필드만 따로 추출해야한다\n",
        "# token embeddings + segment embeddings + positional embeddings"
      ],
      "metadata": {
        "id": "P3Ok9413nzAI"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**"
      ],
      "metadata": {
        "id": "R_dKfY6TnEgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, input_size = 8 , embedding_dim = 128 , num_heads = 8):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embedding_dim // num_heads\n",
        "    self.input_size = input_size\n",
        "\n",
        "    self.q = nn.Linear(input_size , embedding_dim)\n",
        "    self.k = nn.Linear(input_size , embedding_dim)\n",
        "    self.v = nn.Linear(input_size , embedding_dim)\n",
        "\n",
        "    self.fc = nn.Linear(self.head_dim , embedding_dim)\n",
        "\n",
        "  def go(self , x):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    q = q.view(batch_size , -1 , self.num_heads , self.head_dim).transpose(1,2) # batch , sequence_length , 8 , 128//8 = 16\n",
        "    k = k.view(batch_size , -1 , self.num_heads , self.head_dim).transpose(1,2)\n",
        "    v = v.view(batch_size , -1 , self.num_heads , self.head_dim).transpose(1,2)\n",
        "\n",
        "    # Scaled Dot-Product Attention\n",
        "    attention_score = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "    attention_score = torch.softmax(attention_score, dim=-1)\n",
        "    attention = torch.matmul(attention_score , v)\n",
        "    attention = attention.transpose(1,2).contiguous().view(batch_size , -1 , self.embedding_dim) # batch , 8 , sequence_length , 128//8 = 16\n",
        "    output = self.fc(attention)\n",
        "\n",
        "    return output # batch , 8 , sequence_length , 512 // 8 = 64"
      ],
      "metadata": {
        "id": "QXl0u5tqnek-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **인풋의 길이가 전부 8로 일정하므로 padding을 추가하지 않는다**"
      ],
      "metadata": {
        "id": "Iz5N10mSG_l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pre_process(nn.Module):\n",
        "    def __init__(self , embed_size = 16, num_heads = 8, vocab_size = 40000 , batch_num = 128 , max_length = 16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # Embedding layer\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(batch_num , embed_size))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def run(self, input , tocken_type_ids , mask = None):\n",
        "        batch_num, seq_length = input.shape\n",
        "        print(0)\n",
        "        out = self.word_embedding(input) + self.position_embedding + token_type_ids  # 단어 임베딩 + position + 문장 소속(binary)\n",
        "        print(1)\n",
        "        out = self.dropout(out)\n",
        "        return out # out.shape : batch_num , embed_size = 128 , 16"
      ],
      "metadata": {
        "id": "fUZxYl6gv5bQ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x -> Encoder -> y -> multiheadattention -> z -> Encoder_block"
      ],
      "metadata": {
        "id": "zuJ9m7TIHi_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder block**"
      ],
      "metadata": {
        "id": "JhwAfQPpI6Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self , embed_size = 128 , num_heads = 8 , dropout = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.l1 = nn.Linear(embed_size , embed_size)\n",
        "    self.l2 = nn.Linear(embed_size , 1)\n",
        "\n",
        "  def update(self , x , input):\n",
        "    #skip connection\n",
        "    #transformer말고 bert식으로 먼저 더하고 layernorm취한다\n",
        "    x = x + input\n",
        "    x = self.norm1(x)\n",
        "\n",
        "    z = self.l1(x)\n",
        "\n",
        "    x = x + z\n",
        "    x = self.l2(x)\n",
        "    x = self.norm2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "y8dGImd3HjJz"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 설계"
      ],
      "metadata": {
        "id": "JMwSEguRJkOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_model = MultiHeadAttention()\n",
        "preprocess_model = Pre_process()\n",
        "encoder = Encoder()"
      ],
      "metadata": {
        "id": "vYGRCRxUJkS-"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self , attention_model , preprocess_model , encoder):\n",
        "    super().__init__()\n",
        "    self.attention_model = attention_model\n",
        "    self.preprocess_model = preprocess_model\n",
        "    self.encoder = encoder\n",
        "\n",
        "  def forward(self , input , token_type_ids):\n",
        "    x_1 = self.preprocess_model.run(input , token_type_ids)\n",
        "    x_2 = self.attention_model.go(x_1)\n",
        "    x_3 = self.encoder.update(x_2 , x_1)\n",
        "    return x_3"
      ],
      "metadata": {
        "id": "vonXSuODwoUR"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **아키텍쳐**"
      ],
      "metadata": {
        "id": "bivAT_HUH-mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(attention_model , preprocess_model , encoder)\n",
        "optimizer =  torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "wDRqwJubLWKw"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "loss_history = []\n",
        "test_loss_history = []\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  for batch in train_loader:\n",
        "\n",
        "    # 필요한 텐서 추출\n",
        "    input_ids = batch['input_ids']\n",
        "    input_ids = torch.stack(input_ids)\n",
        "\n",
        "    # 어디 문장 소속인지 여부를 표시하는 텐서 추가\n",
        "    token_type_ids = batch['token_type_ids']\n",
        "    token_type_ids = torch.stack(token_type_ids)\n",
        "\n",
        "    #label\n",
        "    labels = batch['label']\n",
        "\n",
        "    #print(model.device)\n",
        "    output = model.forward(input_ids , token_type_ids)\n",
        "    loss = ce_loss(output, labels)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # loss history append\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "  #scheduler_linear.step()\n",
        "  print(\"epoch : {} , loss : {}\".format(epoch, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "-prIM1CrHjp0",
        "outputId": "4d173597-463b-483e-994c-23450f85fc4f"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "torch.Size([128, 16, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-bde89f2b8dcf>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#print(model.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-127-cc932cb4b83d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, token_type_ids)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mx_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-64fa2232b963>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input, tocken_type_ids, mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_ids\u001b[0m  \u001b[0;31m# 단어 임베딩 + position + 문장 소속(binary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}