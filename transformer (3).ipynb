{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "c7g43tDcf92S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "from transformers import default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **seed 고정**"
      ],
      "metadata": {
        "id": "P62BYcLRgOcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "hqP9wvkogOm2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU"
      ],
      "metadata": {
        "id": "rrDSkANGKARF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8XUH9ZlKAVV",
        "outputId": "dde4b5fc-bb5a-42ab-e944-36e487c98557"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging face dateset가져오기**"
      ],
      "metadata": {
        "id": "UhR6V876g0H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face datasets 라이브러리 설치 (설치되지 않은 경우)\n",
        "!pip install datasets\n",
        "\n",
        "# GLUE 데이터셋 불러오기\n",
        "from datasets import load_dataset\n",
        "\n",
        "# MRPC (Microsoft Research Paraphrase Corpus) 태스크 로드 예시\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")"
      ],
      "metadata": {
        "id": "cVuuqi9Mg0oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56caaa9-040b-4824-ffef-d3237e4a8c81"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 정보 출력\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_l1c0qFhRWy",
        "outputId": "5b8daa1c-8096-46fb-e3ca-4d55d9b157c2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence1 , sentence2의 동의어 관계 판별 ,\n",
        "\n",
        "---\n",
        "label : 1 -> 동의어 관계\n",
        "label : 0 -> 동의어 관계 아님\n",
        "\n",
        "---\n",
        "idx : 고유 id\n",
        "\n"
      ],
      "metadata": {
        "id": "o-M7clpairzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련, 검증 데이터 예시 출력\n",
        "print(f\"Train Example: {dataset['train'][0]}\")\n",
        "print(f\"Validation Example: {dataset['validation'][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIEBvV71hPv7",
        "outputId": "397b5994-0197-43ff-cd04-a063d9aab840"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Example: {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
            "Validation Example: {'sentence1': \"He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\", 'sentence2': '\" The foodservice pie business does not fit our long-term growth strategy .', 'label': 1, 'idx': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰화**"
      ],
      "metadata": {
        "id": "0ZkG9Grohqw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. BERT 토크나이저 불러오기 (사전 학습된 BERT 모델 사용)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GusHkmBihq6w",
        "outputId": "856bfe51-f8ec-4bc5-b4e6-1ffeffd4444e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 토큰화 함수 정의\n",
        "def tokenize_function(examples):\n",
        "    # sentence1과 sentence2를 함께 토큰화 (Padding, Truncation 처리)\n",
        "    return tokenizer(examples[\"sentence1\"],\n",
        "                     examples[\"sentence2\"],\n",
        "                     padding=\"max_length\",\n",
        "                     truncation=True,\n",
        "                     max_length=16)"
      ],
      "metadata": {
        "id": "RUfi08-xkTpm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 데이터셋에 토큰화 적용\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "DHk1hnGjkVX_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 토큰화된 데이터 예시 출력\n",
        "print(tokenized_dataset[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLrWK7eakcPk",
        "outputId": "c79887b4-ba9b-4a55-b856-90f039f91808"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0, 'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 102, 7727, 2000, 2032, 2004, 2069, 1000, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구조 :\n",
        "\n",
        "---\n",
        "\n",
        "(sentence1 원문 , sentence2 원문 , label , idx , input_ids1 & 2 , token_type_ids , attention_mask)\n",
        "\n",
        "---\n",
        "token_type_ids : 두 문장(문장1, 문장2)을 구분하기 위한 인코딩 (BERT에서 사용),\n",
        "\n",
        "---\n",
        "\n",
        "attention_mask : 패딩된 부분을 무시하도록 마스킹 처리 (1은 유효한 토큰, 0은 패딩)\n"
      ],
      "metadata": {
        "id": "bB3oefQyly8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **배치화**"
      ],
      "metadata": {
        "id": "k5Dnu5gIk2bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 DataLoader에 넣기\n",
        "train_loader = train_loader = DataLoader(tokenized_dataset[\"train\"], batch_size=256, collate_fn=default_data_collator)"
      ],
      "metadata": {
        "id": "KK-gKR9uk8u9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 배치 반복 및 크기 확인\n",
        "for batch in train_loader:\n",
        "    print(batch.keys())  # 딕셔너리의 키 확인 (예: input_ids, attention_mask, label)\n",
        "\n",
        "    input_ids = batch['input_ids']\n",
        "    print(f\"Input IDs shape: {len(input_ids)}\")\n",
        "    print(f\"Input IDs shape: {len(input_ids[0])}\")\n",
        "\n",
        "    attention_mask = batch['attention_mask']\n",
        "    print(f\"Attention Mask shape: {len(attention_mask)}\")\n",
        "    print(f\"Attention Mask shape: {len(attention_mask[0])}\")\n",
        "    break  # 첫 배치만 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSph_f_kk4yZ",
        "outputId": "431ab233-8e9e-457e-9cda-5d194f36165a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['labels', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
            "Input IDs shape: 256\n",
            "Input IDs shape: 16\n",
            "Attention Mask shape: 256\n",
            "Attention Mask shape: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **필드 추출**"
      ],
      "metadata": {
        "id": "gF-Q8ZFE2azs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. DataLoader에서 배치 추출 및 Encoder에 입력 예시\n",
        "for batch in train_loader:\n",
        "    # 필요한 텐서 추출\n",
        "    input_ids = batch['input_ids']\n",
        "    # 어디 문장 소속인지 여부를 표시하는 텐서 추가\n",
        "    token_type_ids = batch['token_type_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "    label = batch['labels']\n",
        "\n",
        "    print(f\"Batch size: {len(input_ids)}, Sequence length: {len(input_ids[0])}\")\n",
        "    print(f\"label : {len(label)}\")"
      ],
      "metadata": {
        "id": "p11P7J3-2aAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f268ab-345e-42ee-cd49-26608ff0f836"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 256, Sequence length: 16\n",
            "label : 256\n",
            "Batch size: 84, Sequence length: 16\n",
            "label : 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터 처리**"
      ],
      "metadata": {
        "id": "Yne_BbbVny3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch는 dictionary 형태이므로 필요한 필드만 따로 추출해야한다\n",
        "# token embeddings + segment embeddings + positional embeddings"
      ],
      "metadata": {
        "id": "P3Ok9413nzAI"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**"
      ],
      "metadata": {
        "id": "R_dKfY6TnEgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **hyperparameter**"
      ],
      "metadata": {
        "id": "xnFoFyjojw29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 16\n",
        "embedding_dim = 16 * num_heads"
      ],
      "metadata": {
        "id": "lNAIj6b5jwfs"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads , embedding_dim , input_size = 16):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embedding_dim // num_heads\n",
        "    self.input_size = input_size\n",
        "\n",
        "    self.q = nn.Linear(embedding_dim , embedding_dim)\n",
        "    self.k = nn.Linear(embedding_dim , embedding_dim)\n",
        "    self.v = nn.Linear(embedding_dim , embedding_dim)\n",
        "\n",
        "    self.fc = nn.Linear(embedding_dim , embedding_dim)\n",
        "\n",
        "  def go(self , x):\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    q = self.q(x) # batch_num , sequence_length , embed_size : 16 , 16 , 128\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    q = q.view(batch_size , -1 , self.num_heads , self.head_dim).transpose(1,2) # batch , num_head, 16 , 16*8//8 = 16\n",
        "    k = k.view(batch_size , -1 , self.num_heads , self.head_dim).transpose(1,2)\n",
        "    v = v.view(batch_size , -1 , self.num_heads , self.head_dim).transpose(1,2)\n",
        "\n",
        "    # Scaled Dot-Product Attention\n",
        "    attention_score = (q @ k.transpose(3,2)) / (self.head_dim ** 0.5)\n",
        "    attention_score = torch.softmax(attention_score, dim=-1)\n",
        "    attention = torch.matmul(attention_score , v)\n",
        "    attention = attention.transpose(1,2).contiguous().view(batch_size , -1 , self.embedding_dim) # batch , 8 , sequence_length , 16*8//8 = 16\n",
        "    attention = attention.reshape(x.shape[0] , x.shape[1] , x.shape[2])\n",
        "    output = self.fc(attention)\n",
        "\n",
        "    return output # batch , sequence_length , 16*8"
      ],
      "metadata": {
        "id": "QXl0u5tqnek-"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **인풋의 길이가 전부 16으로 일정하므로 padding을 추가하지 않는다**"
      ],
      "metadata": {
        "id": "Iz5N10mSG_l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pre_process(nn.Module):\n",
        "    def __init__(self ,num_heads , embedding_dim , vocab_size = 40000 , max_length = 16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_size = embedding_dim\n",
        "        self.max_length = max_length\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1 , max_length , embedding_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def run(self , input , token_type_ids , mask = None):\n",
        "        batch_num , seq_length = input.shape\n",
        "        z = self.word_embedding(input) # z.shape : batch_num , sequence_length , embedding_dim = 16 , 16 , 128\n",
        "        positional_embed = self.position_embedding.expand(batch_num , self.max_length , self.embed_size)\n",
        "        # 단어 임베딩 + position + 문장 소속(binary)\n",
        "        out = z + positional_embed + token_type_ids.unsqueeze(2).expand(batch_num,self.max_length,self.embed_size)\n",
        "        out = self.dropout(out)\n",
        "        return out # out.shape : batch_num , sequence_length , embed_size = 16 , 16 , 128"
      ],
      "metadata": {
        "id": "fUZxYl6gv5bQ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x -> Encoder -> y -> multiheadattention -> z -> Encoder_block"
      ],
      "metadata": {
        "id": "zuJ9m7TIHi_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder block**"
      ],
      "metadata": {
        "id": "JhwAfQPpI6Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Block(nn.Module):\n",
        "  def __init__(self, num_heads , embed_size , dropout = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(1)\n",
        "\n",
        "    self.l1 = nn.Linear(embed_size , embed_size)\n",
        "    self.l2 = nn.Linear(embed_size , 1)\n",
        "    self.l3 = nn.Linear(16 , 1)\n",
        "\n",
        "    self.pre_process = Pre_process(num_heads , embed_size)\n",
        "    self.attention = MultiHeadAttention(num_heads , embed_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def update(self , input , token_type_ids):\n",
        "\n",
        "    c = self.pre_process.run(input , token_type_ids)\n",
        "    z = self.attention.go(c)\n",
        "\n",
        "    z = z + c\n",
        "    # print(z.shape) , [16, 16, 128]\n",
        "    z1 = self.norm1(z)\n",
        "\n",
        "    z = self.l1(z1)\n",
        "    z = self.relu(z)\n",
        "\n",
        "    #skip\n",
        "    z = z1 + z  # z.shape : 16 ,16 , 128\n",
        "\n",
        "    z = self.l2(z) # 16 ,16 , 1\n",
        "    z = z.reshape(z.shape[0] , 16)\n",
        "    z = self.l3(z) # 16 , 1\n",
        "\n",
        "    z = self.norm2(z) # 16 , 1\n",
        "\n",
        "    return z # 16 , 1"
      ],
      "metadata": {
        "id": "y8dGImd3HjJz"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 설계"
      ],
      "metadata": {
        "id": "JMwSEguRJkOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Encoder_Block(num_heads , embedding_dim)"
      ],
      "metadata": {
        "id": "vYGRCRxUJkS-"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **아키텍쳐**"
      ],
      "metadata": {
        "id": "bivAT_HUH-mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = model.to(device)\n",
        "optimizer =  torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "wDRqwJubLWKw"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "loss_history = []\n",
        "test_loss_history = []\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  for batch in train_loader:\n",
        "\n",
        "    # 필요한 텐서 추출\n",
        "    input_ids = batch['input_ids']\n",
        "    #input_ids = torch.stack(input_ids)\n",
        "\n",
        "    # 어디 문장 소속인지 여부를 표시하는 텐서 추가\n",
        "    token_type_ids = batch['token_type_ids']\n",
        "    #token_type_ids = torch.stack(token_type_ids)\n",
        "\n",
        "    #label\n",
        "    labels = batch['labels'].unsqueeze(1)\n",
        "\n",
        "    #print(model.device)\n",
        "    output = model.update(input_ids , token_type_ids)\n",
        "    loss = bce_loss(output, labels.float())\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # loss history append\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "  #scheduler_linear.step()\n",
        "  print(\"epoch : {} , loss : {}\".format(epoch, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "-prIM1CrHjp0",
        "outputId": "91991761-9474-483d-efd4-e8b29d1bc218"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 , loss : 0.6647324562072754\n",
            "epoch : 1 , loss : 0.6406077146530151\n",
            "epoch : 2 , loss : 0.6233289837837219\n",
            "epoch : 3 , loss : 0.6117141842842102\n",
            "epoch : 4 , loss : 0.6042519807815552\n",
            "epoch : 5 , loss : 0.5996034145355225\n",
            "epoch : 6 , loss : 0.5967675447463989\n",
            "epoch : 7 , loss : 0.5950652956962585\n",
            "epoch : 8 , loss : 0.5940607786178589\n",
            "epoch : 9 , loss : 0.5934818983078003\n",
            "epoch : 10 , loss : 0.5931591987609863\n",
            "epoch : 11 , loss : 0.5929873585700989\n",
            "epoch : 12 , loss : 0.5929015278816223\n",
            "epoch : 13 , loss : 0.592862069606781\n",
            "epoch : 14 , loss : 0.5928464531898499\n",
            "epoch : 15 , loss : 0.5928419232368469\n",
            "epoch : 16 , loss : 0.592842161655426\n",
            "epoch : 17 , loss : 0.5928438901901245\n",
            "epoch : 18 , loss : 0.5928460359573364\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-6ccd94597248>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# loss history append\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "초기 , num_heads = 8 , lr = 0.001\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "epoch99 : 0.594"
      ],
      "metadata": {
        "id": "kQ_lo7-JjHcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning_rate from 0.001 to 0.01\n",
        "\n",
        "---\n",
        "epcoh99 : 0.592\n"
      ],
      "metadata": {
        "id": "NfL1XXNUjlqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr : 0.005 , num_heads = 16"
      ],
      "metadata": {
        "id": "3lAgm_pzkfC9"
      }
    }
  ]
}